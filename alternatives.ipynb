{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33c962b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torchmetrics #when using google colab extension enable this line\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368ae8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from tqdm import tqdm\n",
    "import torchmetrics\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28af451",
   "metadata": {},
   "outputs": [],
   "source": [
    "device= torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec0f8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396ce1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)), # Resize all images to a uniform size (e.g., 64x64 pixels)\n",
    "    transforms.ToTensor(), # Convert PIL Image to PyTorch tensor (0-255 to 0.0-1.0)\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) # Normalize pixel values\n",
    "])\n",
    "\n",
    "train_data_dir = 'dataset/Human Faces Dataset' \n",
    "full_dataset = torchvision.datasets.ImageFolder(root=train_data_dir, transform=transform)\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(full_dataset, [0.7, 0.3])\n",
    "\n",
    "\n",
    "print(dict(Counter(train_dataset.dataset.targets)))\n",
    "\n",
    "print(dict(Counter(test_dataset.dataset.targets)))\n",
    "print(dict(Counter(full_dataset.targets)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c04392",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True, # Shuffle data for training\n",
    "    num_workers=2 # Use multiple subprocesses for data loading \n",
    ")\n",
    "\n",
    "# get some random training images\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "imshow(torchvision.utils.make_grid(images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b524cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the targets for the subset\n",
    "subset_targets = [full_dataset.targets[i] for i in test_dataset.indices]\n",
    "\n",
    "# Calculate the class distribution\n",
    "class_distribution = Counter(subset_targets)\n",
    "class_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd26d34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True, # Shuffle data for training\n",
    "    num_workers=2 # Use multiple subprocesses for data loading \n",
    ")\n",
    "\n",
    "# get some random training images\n",
    "dataiter = iter(test_loader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "imshow(torchvision.utils.make_grid(images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0d29e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sanity check (set shuffle=False in loader so order in file explorer is perserved)\n",
    "index=0\n",
    "npimg=train_loader.dataset[index][0].numpy()\n",
    "plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "plt.show()\n",
    "train_loader.dataset[index][1]\n",
    "\n",
    "img, label=train_loader.dataset[index]\n",
    "img.size()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f7c7bd",
   "metadata": {},
   "source": [
    "<h2> FEATURE EXTRACTION FUNCTION </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f97472f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# function to extract features \n",
    "def extract_features(data_loader):\n",
    "    #iterates through DataLoader and flattens image tensors\n",
    "    all_features = []\n",
    "    all_labels = []\n",
    "\n",
    "    for images, labels in data_loader:\n",
    "        # flatten the image data: C * H * W\n",
    "        # resulting shape: (Batch, Features)\n",
    "        flattened_images = images.reshape(images.shape[0], -1)\n",
    "\n",
    "        # convert to numpy and append\n",
    "        all_features.append(flattened_images.numpy())\n",
    "        all_labels.append(labels.numpy())\n",
    "\n",
    "    # concatenates all batches into single numpy arrays\n",
    "    X = np.concatenate(all_features, axis=0)\n",
    "    y = np.concatenate(all_labels, axis=0)\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e19ace1",
   "metadata": {},
   "source": [
    "<h2> LP </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b505a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_loader = DataLoader(full_dataset, batch_size=64, shuffle=False)\n",
    "print(\"Processing all image data...\")\n",
    "X_full, y_true_full = extract_features(full_loader)\n",
    "print(f\"Feature Matrix Shape (X_full): {X_full.shape}\")\n",
    "\n",
    "train_indices = train_dataset.indices\n",
    "all_indices = np.arange(len(full_dataset))\n",
    "\n",
    "\n",
    "y_lp = np.full(len(full_dataset), -1)\n",
    "y_lp[train_indices] = y_true_full[train_indices] # now y_lp has true labels (0-9) for training set samples, -1 for test set samples\n",
    "# X_full contains the features for ALL samples\n",
    "# y_lp contains labels for labeled samples and -1 for unlabeled samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718e2857",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.semi_supervised import LabelPropagation\n",
    "\n",
    "\n",
    "print(\"\\nStarting Label Propagation...\")\n",
    "label_propagator = LabelPropagation(kernel='knn', n_neighbors=7)\n",
    "label_propagator.fit(X_full, y_lp)\n",
    "print(\"Label Propagation complete.\")\n",
    "\n",
    "\n",
    "# prediction is the output of fit\n",
    "test_indices = test_dataset.indices\n",
    "y_pred_lp = label_propagator.transduction_[test_indices]\n",
    "y_true_test = y_true_full[test_indices]\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(\"Label Propagation Classification Report\")\n",
    "print(classification_report(y_true_test, y_pred_lp))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb188623",
   "metadata": {},
   "source": [
    "<!-- <h2> GB </h2> -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483ae7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Processing Training Data...\")\n",
    "X_train, y_train = extract_features(train_loader)\n",
    "\n",
    "print(\"Processing Testing Data...\")\n",
    "X_test, y_test = extract_features(test_loader)\n",
    "\n",
    "print(f\"\\nFinal X_train shape for GB: {X_train.shape}\")\n",
    "print(f\"Final X_test shape for GB: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22cea80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "# SINGLE DECISION TREE\n",
    "\n",
    "dt = tree.DecisionTreeClassifier()\n",
    "dt = dt.fit(X_train,y_train)\n",
    "\n",
    "y_predictions=dt.predict(X_test)\n",
    "\n",
    "print(\"SINGLE DECISION TREE\")\n",
    "print(classification_report(y_test,y_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e2ab9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# gb = GradientBoostingClassifier()\n",
    "\n",
    "# gb.fit(X_train,y_train)\n",
    "\n",
    "# y_predictions=gb.predict(X_test)\n",
    "\n",
    "# print(\"GRADIENT BOOSTING\")\n",
    "# print(classification_report(y_test,y_predictions))\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
